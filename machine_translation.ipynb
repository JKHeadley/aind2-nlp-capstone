{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Artificial Intelligence Nanodegree\n",
    "## Machine Translation Project\n",
    "In this notebook, sections that end with **'(IMPLEMENTATION)'** in the header indicate that the following blocks of code will require additional functionality which you must provide. Please be sure to read the instructions carefully!\n",
    "\n",
    "## Introduction\n",
    "In this notebook, you will build a deep neural network that functions as part of an end-to-end machine translation pipeline. Your completed pipeline will accept English text as input and return the French translation.\n",
    "\n",
    "- **Preprocess** - You'll convert text to sequence of integers.\n",
    "- **Models** Create models which accepts a sequence of integers as input and returns a probability distribution over possible translations. After learning about the basic types of neural networks that are often used for machine translation, you will engage in your own investigations, to design your own model!\n",
    "- **Prediction** Run the model on English text.\n",
    "\n",
    "## Dataset\n",
    "We begin by investigating the dataset that will be used to train and evaluate your pipeline.  The most common datasets used for machine translation are from [WMT](http://www.statmt.org/).  However, that will take a long time to train a neural network on.  We'll be using a dataset we created for this project that contains a small vocabulary.  You'll be able to train your model in a reasonable time with this dataset.\n",
    "### Load Data\n",
    "The data is located in `data/small_vocab_en` and `data/small_vocab_fr`. The `small_vocab_en` file contains English sentences with their French translations in the `small_vocab_fr` file. Load the English and French data from these files from running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "import helper\n",
    "\n",
    "\n",
    "# Load English data\n",
    "english_sentences = helper.load_data('data/small_vocab_en')\n",
    "# Load French data\n",
    "french_sentences = helper.load_data('data/small_vocab_fr')\n",
    "\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files\n",
    "Each line in `small_vocab_en` contains an English sentence with the respective translation in each line of `small_vocab_fr`.  View the first two lines from each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_vocab_en Line 1:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "small_vocab_fr Line 1:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "small_vocab_en Line 2:  the united states is usually chilly during july , and it is usually freezing in november .\n",
      "small_vocab_fr Line 2:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(2):\n",
    "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
    "    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at the sentences, you can see they have been preprocessed already.  The puncuations have been delimited using spaces. All the text have been converted to lowercase.  This should save you some time, but the text requires more preprocessing.\n",
    "### Vocabulary\n",
    "The complexity of the problem is determined by the complexity of the vocabulary.  A more complex vocabulary is a more complex problem.  Let's look at the complexity of the dataset we'll be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1823250 English words.\n",
      "227 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"is\" \",\" \".\" \"in\" \"it\" \"during\" \"the\" \"but\" \"and\" \"sometimes\"\n",
      "\n",
      "1961295 French words.\n",
      "355 unique French words.\n",
      "10 Most common words in the French dataset:\n",
      "\"est\" \".\" \",\" \"en\" \"il\" \"les\" \"mais\" \"et\" \"la\" \"parfois\"\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "\n",
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
    "print('{} unique French words.'.format(len(french_words_counter)))\n",
    "print('10 Most common words in the French dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, _Alice's Adventures in Wonderland_ contains 2,766 unique words of a total of 15,500 words.\n",
    "## Preprocess\n",
    "For this project, you won't use text data as input to your model. Instead, you'll convert the text into sequences of integers using the following preprocess methods:\n",
    "1. Tokenize the words into ids\n",
    "2. Add padding to make all the sequences the same length.\n",
    "\n",
    "Time to start preprocessing the data...\n",
    "### Tokenize (IMPLEMENTATION)\n",
    "For a neural network to predict on text data, it first has to be turned into data it can understand. Text data like \"dog\" is a sequence of ASCII character encodings.  Since a neural network is a series of multiplication and addition operations, the input data needs to be number(s).\n",
    "\n",
    "We can turn each character into a number or each word into a number.  These are called character and word ids, respectively.  Character ids are used for character level models that generate text predictions for each character.  A word level model uses word ids that generate text predictions for each word.  Word level models tend to learn better, since they are lower in complexity, so we'll use those.\n",
    "\n",
    "Turn each sentence into a sequence of words ids using Keras's [`Tokenizer`](https://keras.io/preprocessing/text/#tokenizer) function. Use this function to tokenize `english_sentences` and `french_sentences` in the cell below.\n",
    "\n",
    "Running the cell will run `tokenize` on sample data and show output for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'study': 13, 'brown': 4, 'a': 3, 'lazy': 8, 'the': 1, 'fox': 5, 'is': 19, 'of': 14, 'by': 10, 'short': 20, 'sentence': 21, 'quick': 2, 'my': 12, 'over': 7, 'won': 16, 'jove': 11, 'this': 18, 'jumps': 6, 'lexicography': 15, 'dog': 9, 'prize': 17}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  The quick brown fox jumps over the lazy dog .\n",
      "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
      "Sequence 2 in x\n",
      "  Input:  By Jove , my quick study of lexicography won a prize .\n",
      "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
      "Sequence 3 in x\n",
      "  Input:  This is a short sentence .\n",
      "  Output: [18, 19, 3, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "import project_tests as tests\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    tokenized_x = tokenizer.texts_to_sequences(x)\n",
    "    return tokenized_x, tokenizer\n",
    "tests.test_tokenize(tokenize)\n",
    "\n",
    "# Tokenize Example output\n",
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .']\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding (IMPLEMENTATION)\n",
    "When batching the sequence of word ids together, each sequence needs to be the same length.  Since sentences are dynamic in length, we can add padding to the end of the sequences to make them the same length.\n",
    "\n",
    "Make sure all the English sequences have the same length and all the French sequences have the same length by adding padding to the **end** of each sequence using Keras's [`pad_sequences`](https://keras.io/preprocessing/sequence/#pad_sequences) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "  Input:  [1 2 4 5 6 7 1 8 9]\n",
      "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
      "Sequence 2 in x\n",
      "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
      "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
      "Sequence 3 in x\n",
      "  Input:  [18 19  3 20 21]\n",
      "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    max_length = len(max(x, key=lambda x: len(x)))\n",
    "    \n",
    "    if (length == None):\n",
    "        length = max_length\n",
    "                     \n",
    "    padded = pad_sequences(x, maxlen=length, padding='post')\n",
    "    \n",
    "    return np.array(padded)\n",
    "\n",
    "\n",
    "tests.test_pad(pad)\n",
    "\n",
    "# Pad Tokenized output\n",
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Pipeline\n",
    "Your focus for this project is to build neural network architecture, so we won't ask you to create a preprocess pipeline.  Instead, we've provided you with the implementation of the `preprocess` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(english_sentences, french_sentences)\n",
    "\n",
    "print('Data Preprocessed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "In this section, you will experiment with various neural network architectures.\n",
    "You will begin by training four relatively simple architectures.\n",
    "- Model 1 is a simple RNN\n",
    "- Model 2 is a RNN with Embedding\n",
    "- Model 3 is a Bidirectional RNN\n",
    "- Model 4 is an optional Encoder-Decoder RNN\n",
    "\n",
    "After experimenting with the four simple architectures, you will construct a deeper architecture that is designed to outperform all four models.\n",
    "### Ids Back to Text\n",
    "The neural network will be translating the input to words ids, which isn't the final form we want.  We want the French translation.  The function `logits_to_text` will bridge the gab between the logits from the neural network to the French translation.  You'll be using this function to better understand the output of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: RNN (IMPLEMENTATION)\n",
    "![RNN](images/rnn.png)\n",
    "A basic RNN model is a good baseline for sequence data.  In this model, you'll build a RNN that translates English to French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SHAPE (21, 1)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 21, 21)            1932      \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 21, 344)           7568      \n",
      "=================================================================\n",
      "Total params: 9,500\n",
      "Trainable params: 9,500\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "INPUT SHAPE (137861, 21)\n",
      "INPUT SHAPE (21, 1)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 21, 21)            1932      \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 21, 345)           7590      \n",
      "=================================================================\n",
      "Total params: 9,522\n",
      "Trainable params: 9,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GRU, Input, Dense, TimeDistributed, LSTM, Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a basic RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"INPUT SHAPE\", input_shape[1:])\n",
    "    # TODO: Build the layers\n",
    "#     input_sequences = Input(shape=input_shape)\n",
    "#     model = GRU(output_sequence_length, return_sequences=True)(input_sequences)\n",
    "#     model = TimeDistributed(Dense(1))(model)\n",
    "    \n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(output_sequence_length, input_shape=input_shape, return_sequences=True))\n",
    "#     model.add(TimeDistributed(Dense(1)))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(output_sequence_length, input_shape=input_shape[1:], return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size)))\n",
    "    print(model.summary())\n",
    "\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "tests.test_simple_model(simple_model)\n",
    "\n",
    "\n",
    "# Reshaping the input to work with a basic RNN\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "print(\"INPUT SHAPE\", tmp_x.shape)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "# Train the neural network\n",
    "simple_rnn_model = simple_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index) + 1,\n",
    "    len(french_tokenizer.word_index) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Print prediction(s)\n",
    "# sentence_index = 5\n",
    "# print(\"ENGLISH:\", english_sentences[sentence_index])\n",
    "# print(\"FRENCH:\", logits_to_text(simple_rnn_model.predict(tmp_x[sentence_index:sentence_index + 1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Embedding (IMPLEMENTATION)\n",
    "![RNN](images/embedding.png)\n",
    "You've turned the words into ids, but there's a better representation of a word.  This is called word embeddings.  An embedding is a vector representation of the word that is close to similar words in n-dimensional space, where the n represents the size of the embedding vectors.\n",
    "\n",
    "In this model, you'll create a RNN model using embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 21, 10)            1990      \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 21, 21)            2688      \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 21, 344)           7568      \n",
      "=================================================================\n",
      "Total params: 12,246\n",
      "Trainable params: 12,246\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 21, 10)            2000      \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 21, 21)            2688      \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 21, 345)           7590      \n",
      "=================================================================\n",
      "Total params: 12,278\n",
      "Trainable params: 12,278\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "\n",
    "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a RNN model using word embedding on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(english_vocab_size, 10, input_length=input_shape[1]))\n",
    "    model.add(LSTM(output_sequence_length, input_shape=input_shape[1:], return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size)))\n",
    "    print(model.summary())\n",
    "\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "tests.test_embed_model(embed_model)\n",
    "\n",
    "\n",
    "# TODO: Reshape the input\n",
    "\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "\n",
    "# TODO: Train the neural network\n",
    "embed_rnn_model = embed_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index) + 1,\n",
    "    len(french_tokenizer.word_index) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Print prediction(s)\n",
    "# sentence_index = 5\n",
    "# print(\"ENGLISH:\", english_sentences[sentence_index])\n",
    "# print(\"FRENCH:\", logits_to_text(embed_rnn_model.predict(tmp_x[sentence_index:sentence_index + 1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Bidirectional RNNs (IMPLEMENTATION)\n",
    "![RNN](images/bidirectional.png)\n",
    "One restriction of a RNN is that it can't see the future input, only the past.  This is where bidirectional recurrent neural networks come in.  They are able to see the future data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 21, 21)            3864      \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 21, 344)           7568      \n",
      "=================================================================\n",
      "Total params: 11,432\n",
      "Trainable params: 11,432\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_2 (Bidirection (None, 21, 21)            3864      \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 21, 345)           7590      \n",
      "=================================================================\n",
      "Total params: 11,454\n",
      "Trainable params: 11,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional\n",
    "\n",
    "\n",
    "def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a bidirectional RNN model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(output_sequence_length, input_shape=input_shape[1:], return_sequences=True), \n",
    "                            input_shape=input_shape[1:], merge_mode='sum'))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size)))\n",
    "    print(model.summary())\n",
    "\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "tests.test_bd_model(bd_model)\n",
    "\n",
    "\n",
    "# TODO: Train and Print prediction(s)\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "# TODO: Train the neural network\n",
    "bd_rnn_model = bd_model(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index) + 1,\n",
    "    len(french_tokenizer.word_index) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bd_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Print prediction(s)\n",
    "# sentence_index = 5\n",
    "# print(\"ENGLISH:\", english_sentences[sentence_index])\n",
    "# print(\"FRENCH:\", logits_to_text(bd_rnn_model.predict(tmp_x[sentence_index:sentence_index + 1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Encoder-Decoder (OPTIONAL)\n",
    "Time to look at encoder-decoder models.  This model is made up of an encoder and decoder. The encoder creates a matrix representation of the sentence.  The decoder takes this matrix as input and predicts the translation as output.\n",
    "\n",
    "Create an encoder-decoder model in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import RepeatVector\n",
    "\n",
    "\n",
    "def encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train an encoder-decoder model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # OPTIONAL: Implement\n",
    "    return None\n",
    "# tests.test_encdec_model(encdec_model)\n",
    "\n",
    "\n",
    "# OPTIONAL: Train and Print prediction(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Custom (IMPLEMENTATION)\n",
    "Use everything you learned from the previous models to create a model that incorporates embedding and a bidirectional rnn into one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Loaded\n"
     ]
    }
   ],
   "source": [
    "def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    \n",
    "    # model1\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(english_vocab_size, 10, input_length=input_shape[1]))\n",
    "#     model.add(Bidirectional(LSTM(output_sequence_length, input_shape=input_shape[1:], return_sequences=True), \n",
    "#                             input_shape=input_shape[1:], merge_mode='sum'))\n",
    "#     model.add(TimeDistributed(Dense(french_vocab_size)))\n",
    "\n",
    "    #model2 - lr = 0.0005\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(english_vocab_size, 500, input_length=input_shape[1]))\n",
    "#     model.add(Bidirectional(LSTM(output_sequence_length, input_shape=input_shape[1:], return_sequences=True), \n",
    "#                             input_shape=input_shape[1:], merge_mode='sum'))\n",
    "#     model.add(Bidirectional(LSTM(output_sequence_length, input_shape=input_shape[1:], return_sequences=True), \n",
    "#                             input_shape=input_shape[1:], merge_mode='sum'))\n",
    "#     model.add(Bidirectional(LSTM(output_sequence_length, input_shape=input_shape[1:], return_sequences=True), \n",
    "#                             input_shape=input_shape[1:], merge_mode='sum'))\n",
    "#     model.add(Bidirectional(LSTM(output_sequence_length, input_shape=input_shape[1:], return_sequences=True), \n",
    "#                             input_shape=input_shape[1:], merge_mode='sum'))\n",
    "#     model.add(TimeDistributed(Dense(french_vocab_size)))\n",
    "    \n",
    "    #model3 - lr = 0.0001\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(english_vocab_size, 500, input_length=input_shape[1]))\n",
    "#     model.add(Bidirectional(GRU(256, input_shape=input_shape[1:], return_sequences=True), \n",
    "#                             input_shape=input_shape[1:], merge_mode='sum'))\n",
    "#     model.add(Bidirectional(GRU(256, input_shape=input_shape[1:], return_sequences=True), \n",
    "#                             input_shape=input_shape[1:], merge_mode='sum'))\n",
    "#     model.add(TimeDistributed(Dense(french_vocab_size)))\n",
    "    \n",
    "    #model4\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(english_vocab_size, 500, input_length=input_shape[1]))\n",
    "#     model.add(Bidirectional(GRU(256, input_shape=input_shape[1:], return_sequences=True), \n",
    "#                             input_shape=input_shape[1:], merge_mode='sum'))\n",
    "#     model.add(Bidirectional(GRU(256, input_shape=input_shape[1:], return_sequences=True), \n",
    "#                             input_shape=input_shape[1:], merge_mode='sum'))\n",
    "#     model.add(Bidirectional(GRU(256, input_shape=input_shape[1:], return_sequences=True), \n",
    "#                             input_shape=input_shape[1:], merge_mode='sum'))\n",
    "#     model.add(Bidirectional(GRU(256, input_shape=input_shape[1:], return_sequences=True), \n",
    "#                             input_shape=input_shape[1:], merge_mode='sum'))\n",
    "#     model.add(TimeDistributed(Dense(french_vocab_size)))\n",
    "    \n",
    "    \n",
    "    #model5\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(english_vocab_size, 500, input_length=input_shape[1]))\n",
    "    model.add(Bidirectional(GRU(512, input_shape=input_shape[1:], return_sequences=True), \n",
    "                            input_shape=input_shape[1:], merge_mode='sum'))\n",
    "    model.add(Bidirectional(GRU(512, input_shape=input_shape[1:], return_sequences=True), \n",
    "                            input_shape=input_shape[1:], merge_mode='sum'))\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size)))\n",
    "    \n",
    "    print(model.summary())\n",
    "\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(lr=0.0001),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "# tests.test_model_final(model_final)\n",
    "\n",
    "\n",
    "print('Final Model Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 21, 500)           100000    \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 21, 512)           3111936   \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 21, 512)           3148800   \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 21, 345)           176985    \n",
      "=================================================================\n",
      "Total params: 6,537,721\n",
      "Trainable params: 6,537,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# TODO: Reshape the input\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "\n",
    "# TODO: Train the neural network\n",
    "model_rnn_final = model_final(\n",
    "    tmp_x.shape,\n",
    "    preproc_french_sentences.shape[1],\n",
    "    len(english_tokenizer.word_index) + 1,\n",
    "    len(french_tokenizer.word_index) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 2.9712 - acc: 0.3597Epoch 00000: val_loss improved from inf to 1.88170, saving model to saved_models/weights.model_final5.hdf5\n",
      "110288/110288 [==============================] - 127s - loss: 2.9689 - acc: 0.3602 - val_loss: 1.8817 - val_acc: 0.5669\n",
      "Epoch 2/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 1.4675 - acc: 0.6031Epoch 00001: val_loss improved from 1.88170 to 1.60118, saving model to saved_models/weights.model_final5.hdf5\n",
      "110288/110288 [==============================] - 127s - loss: 1.4678 - acc: 0.6029 - val_loss: 1.6012 - val_acc: 0.4980\n",
      "Epoch 3/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 1.2794 - acc: 0.6054Epoch 00002: val_loss improved from 1.60118 to 1.11596, saving model to saved_models/weights.model_final5.hdf5\n",
      "110288/110288 [==============================] - 127s - loss: 1.2792 - acc: 0.6054 - val_loss: 1.1160 - val_acc: 0.6610\n",
      "Epoch 4/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 1.2330 - acc: 0.6258Epoch 00003: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 1.2329 - acc: 0.6258 - val_loss: 1.1640 - val_acc: 0.6494\n",
      "Epoch 5/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 1.1700 - acc: 0.6390Epoch 00004: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 1.1705 - acc: 0.6387 - val_loss: 1.4590 - val_acc: 0.4960\n",
      "Epoch 6/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 1.1071 - acc: 0.6667Epoch 00005: val_loss improved from 1.11596 to 1.01395, saving model to saved_models/weights.model_final5.hdf5\n",
      "110288/110288 [==============================] - 127s - loss: 1.1070 - acc: 0.6668 - val_loss: 1.0139 - val_acc: 0.6919\n",
      "Epoch 7/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 1.2434 - acc: 0.5995Epoch 00006: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 1.2462 - acc: 0.5984 - val_loss: 2.6029 - val_acc: 0.0455\n",
      "Epoch 8/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 1.8214 - acc: 0.1642Epoch 00007: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 1.8207 - acc: 0.1643 - val_loss: 1.3730 - val_acc: 0.2401\n",
      "Epoch 9/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 1.3200 - acc: 0.2563Epoch 00008: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 1.3197 - acc: 0.2563 - val_loss: 1.2839 - val_acc: 0.2773\n",
      "Epoch 10/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 1.5414 - acc: 0.3500Epoch 00009: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 1.5413 - acc: 0.3503 - val_loss: 1.5519 - val_acc: 0.4876\n",
      "Epoch 11/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 1.6284 - acc: 0.4995Epoch 00010: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 1.6282 - acc: 0.4996 - val_loss: 1.4927 - val_acc: 0.5521\n",
      "Epoch 12/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 1.5364 - acc: 0.5197Epoch 00011: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 1.5365 - acc: 0.5196 - val_loss: 1.5318 - val_acc: 0.5022\n",
      "Epoch 13/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 1.3466 - acc: 0.6143Epoch 00012: val_loss improved from 1.01395 to 1.00311, saving model to saved_models/weights.model_final5.hdf5\n",
      "110288/110288 [==============================] - 127s - loss: 1.3458 - acc: 0.6144 - val_loss: 1.0031 - val_acc: 0.6764\n",
      "Epoch 14/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.9731 - acc: 0.6718Epoch 00013: val_loss improved from 1.00311 to 0.95469, saving model to saved_models/weights.model_final5.hdf5\n",
      "110288/110288 [==============================] - 126s - loss: 0.9731 - acc: 0.6718 - val_loss: 0.9547 - val_acc: 0.6813\n",
      "Epoch 15/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.9389 - acc: 0.6748Epoch 00014: val_loss improved from 0.95469 to 0.89216, saving model to saved_models/weights.model_final5.hdf5\n",
      "110288/110288 [==============================] - 126s - loss: 0.9386 - acc: 0.6748 - val_loss: 0.8922 - val_acc: 0.6702\n",
      "Epoch 16/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.9716 - acc: 0.6548Epoch 00015: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.9731 - acc: 0.6541 - val_loss: 1.7541 - val_acc: 0.2961\n",
      "Epoch 17/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 1.4153 - acc: 0.4872Epoch 00016: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 1.4151 - acc: 0.4874 - val_loss: 1.3371 - val_acc: 0.5598\n",
      "Epoch 18/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 1.1615 - acc: 0.6385Epoch 00017: val_loss improved from 0.89216 to 0.81147, saving model to saved_models/weights.model_final5.hdf5\n",
      "110288/110288 [==============================] - 127s - loss: 1.1609 - acc: 0.6386 - val_loss: 0.8115 - val_acc: 0.7060\n",
      "Epoch 19/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8928 - acc: 0.6869Epoch 00018: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.8929 - acc: 0.6869 - val_loss: 0.9882 - val_acc: 0.6528\n",
      "Epoch 20/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 1.0668 - acc: 0.6074Epoch 00019: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 1.0668 - acc: 0.6074 - val_loss: 1.0603 - val_acc: 0.6297\n",
      "Epoch 21/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.9222 - acc: 0.6624Epoch 00020: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.9219 - acc: 0.6624 - val_loss: 0.8277 - val_acc: 0.6986\n",
      "Epoch 22/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.9570 - acc: 0.6449Epoch 00021: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.9568 - acc: 0.6450 - val_loss: 0.9276 - val_acc: 0.6799\n",
      "Epoch 23/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8581 - acc: 0.6907Epoch 00022: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.8580 - acc: 0.6908 - val_loss: 0.8115 - val_acc: 0.6959\n",
      "Epoch 24/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8758 - acc: 0.6898Epoch 00023: val_loss improved from 0.81147 to 0.79253, saving model to saved_models/weights.model_final5.hdf5\n",
      "110288/110288 [==============================] - 127s - loss: 0.8758 - acc: 0.6898 - val_loss: 0.7925 - val_acc: 0.7039\n",
      "Epoch 25/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7782 - acc: 0.7131Epoch 00024: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7784 - acc: 0.7130 - val_loss: 0.8770 - val_acc: 0.6820\n",
      "Epoch 26/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8579 - acc: 0.6816Epoch 00025: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.8584 - acc: 0.6815 - val_loss: 1.0389 - val_acc: 0.6300\n",
      "Epoch 27/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8659 - acc: 0.6746Epoch 00026: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.8659 - acc: 0.6747 - val_loss: 0.8241 - val_acc: 0.6884\n",
      "Epoch 28/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7881 - acc: 0.7064Epoch 00027: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7881 - acc: 0.7064 - val_loss: 0.7953 - val_acc: 0.7208\n",
      "Epoch 29/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7888 - acc: 0.7156Epoch 00028: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7895 - acc: 0.7153 - val_loss: 1.1204 - val_acc: 0.5586\n",
      "Epoch 30/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8234 - acc: 0.6932Epoch 00029: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.8234 - acc: 0.6932 - val_loss: 0.8417 - val_acc: 0.6987\n",
      "Epoch 31/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8052 - acc: 0.7076Epoch 00030: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.8050 - acc: 0.7076 - val_loss: 0.8008 - val_acc: 0.7132\n",
      "Epoch 32/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.9492 - acc: 0.6530Epoch 00031: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.9488 - acc: 0.6531 - val_loss: 0.8248 - val_acc: 0.7047\n",
      "Epoch 33/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7938 - acc: 0.7096Epoch 00032: val_loss improved from 0.79253 to 0.76329, saving model to saved_models/weights.model_final5.hdf5\n",
      "110288/110288 [==============================] - 127s - loss: 0.7936 - acc: 0.7096 - val_loss: 0.7633 - val_acc: 0.7207\n",
      "Epoch 34/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7682 - acc: 0.7127Epoch 00033: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7684 - acc: 0.7127 - val_loss: 0.8434 - val_acc: 0.7137\n",
      "Epoch 35/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.9806 - acc: 0.6326Epoch 00034: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.9801 - acc: 0.6328 - val_loss: 0.7636 - val_acc: 0.7163\n",
      "Epoch 36/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.9219 - acc: 0.6480Epoch 00035: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.9218 - acc: 0.6480 - val_loss: 0.8433 - val_acc: 0.6765\n",
      "Epoch 37/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8003 - acc: 0.6857Epoch 00036: val_loss improved from 0.76329 to 0.75490, saving model to saved_models/weights.model_final5.hdf5\n",
      "110288/110288 [==============================] - 127s - loss: 0.8002 - acc: 0.6857 - val_loss: 0.7549 - val_acc: 0.7064\n",
      "Epoch 38/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7453 - acc: 0.7132Epoch 00037: val_loss improved from 0.75490 to 0.73374, saving model to saved_models/weights.model_final5.hdf5\n",
      "110288/110288 [==============================] - 127s - loss: 0.7454 - acc: 0.7132 - val_loss: 0.7337 - val_acc: 0.7186\n",
      "Epoch 39/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7116 - acc: 0.7263Epoch 00038: val_loss improved from 0.73374 to 0.70847, saving model to saved_models/weights.model_final5.hdf5\n",
      "110288/110288 [==============================] - 127s - loss: 0.7116 - acc: 0.7263 - val_loss: 0.7085 - val_acc: 0.7319\n",
      "Epoch 40/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8181 - acc: 0.6901Epoch 00039: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.8179 - acc: 0.6902 - val_loss: 0.7364 - val_acc: 0.7266\n",
      "Epoch 41/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7178 - acc: 0.7290Epoch 00040: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.7179 - acc: 0.7289 - val_loss: 0.7146 - val_acc: 0.7269\n",
      "Epoch 42/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7940 - acc: 0.7084Epoch 00041: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.7942 - acc: 0.7084 - val_loss: 0.8430 - val_acc: 0.6863\n",
      "Epoch 43/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7799 - acc: 0.7002Epoch 00042: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7799 - acc: 0.7002 - val_loss: 0.7411 - val_acc: 0.7158\n",
      "Epoch 44/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7323 - acc: 0.7198Epoch 00043: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7323 - acc: 0.7198 - val_loss: 0.7198 - val_acc: 0.7294\n",
      "Epoch 45/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7003 - acc: 0.7336Epoch 00044: val_loss improved from 0.70847 to 0.69077, saving model to saved_models/weights.model_final5.hdf5\n",
      "110288/110288 [==============================] - 127s - loss: 0.7002 - acc: 0.7336 - val_loss: 0.6908 - val_acc: 0.7388\n",
      "Epoch 46/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7199 - acc: 0.7322Epoch 00045: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7199 - acc: 0.7322 - val_loss: 0.7093 - val_acc: 0.7380\n",
      "Epoch 47/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7181 - acc: 0.7337Epoch 00046: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7180 - acc: 0.7338 - val_loss: 0.7055 - val_acc: 0.7405\n",
      "Epoch 48/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.9639 - acc: 0.6023Epoch 00047: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.9638 - acc: 0.6025 - val_loss: 0.8465 - val_acc: 0.6900\n",
      "Epoch 49/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8160 - acc: 0.7165Epoch 00048: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.8160 - acc: 0.7165 - val_loss: 0.7588 - val_acc: 0.7421\n",
      "Epoch 50/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.9961 - acc: 0.6495Epoch 00049: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.9959 - acc: 0.6495 - val_loss: 0.8684 - val_acc: 0.6931\n",
      "Epoch 51/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7876 - acc: 0.7174Epoch 00050: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7878 - acc: 0.7173 - val_loss: 0.9381 - val_acc: 0.6800\n",
      "Epoch 52/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7794 - acc: 0.7224Epoch 00051: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7793 - acc: 0.7224 - val_loss: 0.7453 - val_acc: 0.7254\n",
      "Epoch 53/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7663 - acc: 0.7185Epoch 00052: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.7661 - acc: 0.7185 - val_loss: 0.7381 - val_acc: 0.7312\n",
      "Epoch 54/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7789 - acc: 0.7100Epoch 00053: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7795 - acc: 0.7098 - val_loss: 1.1965 - val_acc: 0.6023\n",
      "Epoch 55/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.9931 - acc: 0.6414Epoch 00054: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.9928 - acc: 0.6415 - val_loss: 0.8626 - val_acc: 0.6817\n",
      "Epoch 56/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7928 - acc: 0.7048Epoch 00055: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7925 - acc: 0.7049 - val_loss: 0.7396 - val_acc: 0.7231\n",
      "Epoch 57/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7329 - acc: 0.7283Epoch 00056: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.7328 - acc: 0.7283 - val_loss: 0.7112 - val_acc: 0.7398\n",
      "Epoch 58/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7220 - acc: 0.7376Epoch 00057: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7220 - acc: 0.7376 - val_loss: 0.7053 - val_acc: 0.7448\n",
      "Epoch 59/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7076 - acc: 0.7428Epoch 00058: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7075 - acc: 0.7428 - val_loss: 0.6954 - val_acc: 0.7410\n",
      "Epoch 60/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7979 - acc: 0.6989Epoch 00059: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110288/110288 [==============================] - 127s - loss: 0.7989 - acc: 0.6985 - val_loss: 1.3056 - val_acc: 0.4689\n",
      "Epoch 61/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8781 - acc: 0.6473Epoch 00060: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.8779 - acc: 0.6474 - val_loss: 0.7332 - val_acc: 0.7229\n",
      "Epoch 62/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7943 - acc: 0.7131Epoch 00061: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.7942 - acc: 0.7132 - val_loss: 0.7502 - val_acc: 0.7314\n",
      "Epoch 63/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7938 - acc: 0.7100Epoch 00062: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7938 - acc: 0.7101 - val_loss: 0.7647 - val_acc: 0.7233\n",
      "Epoch 64/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7128 - acc: 0.7383Epoch 00063: val_loss improved from 0.69077 to 0.68525, saving model to saved_models/weights.model_final5.hdf5\n",
      "110288/110288 [==============================] - 127s - loss: 0.7129 - acc: 0.7382 - val_loss: 0.6853 - val_acc: 0.7480\n",
      "Epoch 65/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8502 - acc: 0.6729Epoch 00064: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.8501 - acc: 0.6729 - val_loss: 0.8190 - val_acc: 0.6746\n",
      "Epoch 66/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8109 - acc: 0.6818Epoch 00065: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.8109 - acc: 0.6818 - val_loss: 0.8039 - val_acc: 0.6834\n",
      "Epoch 67/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 1.1215 - acc: 0.3924Epoch 00066: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 1.1216 - acc: 0.3921 - val_loss: 1.1752 - val_acc: 0.2288\n",
      "Epoch 68/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 1.1499 - acc: 0.2679Epoch 00067: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 1.1498 - acc: 0.2680 - val_loss: 1.0991 - val_acc: 0.3018\n",
      "Epoch 69/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 1.0407 - acc: 0.3846Epoch 00068: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 1.0405 - acc: 0.3849 - val_loss: 0.9697 - val_acc: 0.5452\n",
      "Epoch 70/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8063 - acc: 0.6824Epoch 00069: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.8062 - acc: 0.6824 - val_loss: 0.8003 - val_acc: 0.6895\n",
      "Epoch 71/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7667 - acc: 0.7105Epoch 00070: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7666 - acc: 0.7106 - val_loss: 0.7498 - val_acc: 0.7157\n",
      "Epoch 72/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7666 - acc: 0.7084Epoch 00071: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7669 - acc: 0.7084 - val_loss: 0.7866 - val_acc: 0.7070\n",
      "Epoch 73/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7164 - acc: 0.7325Epoch 00072: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7163 - acc: 0.7326 - val_loss: 0.6935 - val_acc: 0.7420\n",
      "Epoch 74/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.9502 - acc: 0.5776Epoch 00073: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.9507 - acc: 0.5770 - val_loss: 1.1960 - val_acc: 0.2705\n",
      "Epoch 75/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 1.4401 - acc: 0.2614Epoch 00074: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 1.4399 - acc: 0.2614 - val_loss: 1.3881 - val_acc: 0.2784\n",
      "Epoch 76/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 1.3098 - acc: 0.2993Epoch 00075: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 1.3098 - acc: 0.2993 - val_loss: 1.2574 - val_acc: 0.3166\n",
      "Epoch 77/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 1.1946 - acc: 0.3278Epoch 00076: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 1.1946 - acc: 0.3278 - val_loss: 1.1546 - val_acc: 0.3391\n",
      "Epoch 78/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 1.1309 - acc: 0.3623Epoch 00077: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 1.1306 - acc: 0.3624 - val_loss: 0.9854 - val_acc: 0.4197\n",
      "Epoch 79/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8555 - acc: 0.6016Epoch 00078: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.8551 - acc: 0.6020 - val_loss: 0.7340 - val_acc: 0.7552\n",
      "Epoch 80/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7121 - acc: 0.7516Epoch 00079: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7128 - acc: 0.7515 - val_loss: 1.0930 - val_acc: 0.6652\n",
      "Epoch 81/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.9091 - acc: 0.6829Epoch 00080: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.9091 - acc: 0.6829 - val_loss: 0.9016 - val_acc: 0.6917\n",
      "Epoch 82/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8244 - acc: 0.6984Epoch 00081: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.8251 - acc: 0.6980 - val_loss: 1.2070 - val_acc: 0.5010\n",
      "Epoch 83/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8940 - acc: 0.6531Epoch 00082: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.8937 - acc: 0.6533 - val_loss: 0.7367 - val_acc: 0.7330\n",
      "Epoch 84/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8032 - acc: 0.7140Epoch 00083: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.8038 - acc: 0.7137 - val_loss: 1.1361 - val_acc: 0.5851\n",
      "Epoch 85/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 1.0776 - acc: 0.5548Epoch 00084: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 1.0773 - acc: 0.5550 - val_loss: 0.9726 - val_acc: 0.6434\n",
      "Epoch 86/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8326 - acc: 0.6807Epoch 00085: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.8325 - acc: 0.6808 - val_loss: 0.7700 - val_acc: 0.7048\n",
      "Epoch 87/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7661 - acc: 0.7092Epoch 00086: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7665 - acc: 0.7092 - val_loss: 0.9507 - val_acc: 0.6697\n",
      "Epoch 88/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7592 - acc: 0.7164Epoch 00087: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7593 - acc: 0.7164 - val_loss: 0.7824 - val_acc: 0.7234\n",
      "Epoch 89/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7260 - acc: 0.7300Epoch 00088: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7259 - acc: 0.7301 - val_loss: 0.6893 - val_acc: 0.7490\n",
      "Epoch 90/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6698 - acc: 0.7528Epoch 00089: val_loss improved from 0.68525 to 0.64874, saving model to saved_models/weights.model_final5.hdf5\n",
      "110288/110288 [==============================] - 127s - loss: 0.6697 - acc: 0.7528 - val_loss: 0.6487 - val_acc: 0.7613\n",
      "Epoch 91/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6493 - acc: 0.7610Epoch 00090: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110288/110288 [==============================] - 126s - loss: 0.6493 - acc: 0.7610 - val_loss: 0.6561 - val_acc: 0.7652\n",
      "Epoch 92/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.9468 - acc: 0.6194Epoch 00091: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.9464 - acc: 0.6195 - val_loss: 0.8134 - val_acc: 0.6609\n",
      "Epoch 93/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7769 - acc: 0.6932Epoch 00092: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.7769 - acc: 0.6932 - val_loss: 0.7363 - val_acc: 0.7053\n",
      "Epoch 94/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7140 - acc: 0.7228Epoch 00093: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7139 - acc: 0.7228 - val_loss: 0.6948 - val_acc: 0.7411\n",
      "Epoch 95/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8346 - acc: 0.6807Epoch 00094: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.8345 - acc: 0.6807 - val_loss: 0.8092 - val_acc: 0.6799\n",
      "Epoch 96/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7691 - acc: 0.7155Epoch 00095: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7690 - acc: 0.7155 - val_loss: 0.7317 - val_acc: 0.7338\n",
      "Epoch 97/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7363 - acc: 0.7271Epoch 00096: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.7363 - acc: 0.7271 - val_loss: 0.6983 - val_acc: 0.7419\n",
      "Epoch 98/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7012 - acc: 0.7346Epoch 00097: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.7012 - acc: 0.7346 - val_loss: 0.6684 - val_acc: 0.7462\n",
      "Epoch 99/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7268 - acc: 0.7278Epoch 00098: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7269 - acc: 0.7278 - val_loss: 0.7460 - val_acc: 0.7446\n",
      "Epoch 100/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6768 - acc: 0.7595Epoch 00099: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.6769 - acc: 0.7594 - val_loss: 0.7292 - val_acc: 0.7379\n",
      "Epoch 101/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6858 - acc: 0.7554Epoch 00100: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6856 - acc: 0.7554 - val_loss: 0.6641 - val_acc: 0.7681\n",
      "Epoch 102/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7653 - acc: 0.7309Epoch 00101: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.7651 - acc: 0.7310 - val_loss: 0.6744 - val_acc: 0.7631\n",
      "Epoch 103/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6646 - acc: 0.7636Epoch 00102: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.6649 - acc: 0.7632 - val_loss: 0.8733 - val_acc: 0.6157\n",
      "Epoch 104/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6569 - acc: 0.7641Epoch 00103: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6571 - acc: 0.7640 - val_loss: 0.7743 - val_acc: 0.7250\n",
      "Epoch 105/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7418 - acc: 0.7362Epoch 00104: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7424 - acc: 0.7360 - val_loss: 0.9978 - val_acc: 0.6510\n",
      "Epoch 106/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8538 - acc: 0.6891Epoch 00105: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.8536 - acc: 0.6892 - val_loss: 0.7351 - val_acc: 0.7264\n",
      "Epoch 107/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6889 - acc: 0.7464Epoch 00106: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6891 - acc: 0.7464 - val_loss: 0.7210 - val_acc: 0.7607\n",
      "Epoch 108/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7582 - acc: 0.7625Epoch 00107: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7583 - acc: 0.7625 - val_loss: 0.7698 - val_acc: 0.7613\n",
      "Epoch 109/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8828 - acc: 0.7225Epoch 00108: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.8836 - acc: 0.7224 - val_loss: 1.1876 - val_acc: 0.6676\n",
      "Epoch 110/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.9605 - acc: 0.6596Epoch 00109: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.9602 - acc: 0.6596 - val_loss: 0.7848 - val_acc: 0.6813\n",
      "Epoch 111/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8030 - acc: 0.6782Epoch 00110: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.8029 - acc: 0.6783 - val_loss: 0.7755 - val_acc: 0.6894\n",
      "Epoch 112/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7152 - acc: 0.7116Epoch 00111: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7151 - acc: 0.7116 - val_loss: 0.6847 - val_acc: 0.7279\n",
      "Epoch 113/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6900 - acc: 0.7334Epoch 00112: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6907 - acc: 0.7333 - val_loss: 1.0897 - val_acc: 0.6588\n",
      "Epoch 114/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8073 - acc: 0.7135Epoch 00113: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.8073 - acc: 0.7135 - val_loss: 0.8262 - val_acc: 0.7041\n",
      "Epoch 115/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8632 - acc: 0.7055Epoch 00114: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.8637 - acc: 0.7054 - val_loss: 1.1652 - val_acc: 0.6397\n",
      "Epoch 116/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8286 - acc: 0.6926Epoch 00115: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.8284 - acc: 0.6927 - val_loss: 0.7275 - val_acc: 0.7188\n",
      "Epoch 117/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6907 - acc: 0.7339Epoch 00116: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.6908 - acc: 0.7340 - val_loss: 0.6728 - val_acc: 0.7468\n",
      "Epoch 118/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6561 - acc: 0.7524Epoch 00117: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6560 - acc: 0.7524 - val_loss: 0.6583 - val_acc: 0.7514\n",
      "Epoch 119/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6673 - acc: 0.7572Epoch 00118: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6672 - acc: 0.7572 - val_loss: 0.6617 - val_acc: 0.7654\n",
      "Epoch 120/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6921 - acc: 0.7563Epoch 00119: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6921 - acc: 0.7563 - val_loss: 0.6864 - val_acc: 0.7571\n",
      "Epoch 121/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7077 - acc: 0.7508Epoch 00120: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7076 - acc: 0.7509 - val_loss: 0.6902 - val_acc: 0.7586\n",
      "Epoch 122/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7622 - acc: 0.7308Epoch 00121: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7620 - acc: 0.7308 - val_loss: 0.6610 - val_acc: 0.7663\n",
      "Epoch 123/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6383 - acc: 0.7708Epoch 00122: val_loss improved from 0.64874 to 0.63618, saving model to saved_models/weights.model_final5.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110288/110288 [==============================] - 127s - loss: 0.6384 - acc: 0.7709 - val_loss: 0.6362 - val_acc: 0.7726\n",
      "Epoch 124/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6812 - acc: 0.7665Epoch 00123: val_loss improved from 0.63618 to 0.61986, saving model to saved_models/weights.model_final5.hdf5\n",
      "110288/110288 [==============================] - 126s - loss: 0.6811 - acc: 0.7665 - val_loss: 0.6199 - val_acc: 0.7821\n",
      "Epoch 125/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6748 - acc: 0.7682Epoch 00124: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6750 - acc: 0.7681 - val_loss: 0.8175 - val_acc: 0.7229\n",
      "Epoch 126/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8627 - acc: 0.6956Epoch 00125: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.8624 - acc: 0.6956 - val_loss: 0.7315 - val_acc: 0.7167\n",
      "Epoch 127/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6857 - acc: 0.7280Epoch 00126: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6856 - acc: 0.7280 - val_loss: 0.6542 - val_acc: 0.7423\n",
      "Epoch 128/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7324 - acc: 0.7269Epoch 00127: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.7327 - acc: 0.7267 - val_loss: 0.9583 - val_acc: 0.6352\n",
      "Epoch 129/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7567 - acc: 0.7133Epoch 00128: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7565 - acc: 0.7133 - val_loss: 0.6932 - val_acc: 0.7394\n",
      "Epoch 130/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7260 - acc: 0.7272Epoch 00129: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.7261 - acc: 0.7271 - val_loss: 0.7436 - val_acc: 0.7134\n",
      "Epoch 131/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7165 - acc: 0.7305Epoch 00130: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.7163 - acc: 0.7306 - val_loss: 0.6727 - val_acc: 0.7486\n",
      "Epoch 132/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6820 - acc: 0.7582Epoch 00131: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.6822 - acc: 0.7582 - val_loss: 0.7058 - val_acc: 0.7618\n",
      "Epoch 133/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6726 - acc: 0.7483Epoch 00132: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6725 - acc: 0.7482 - val_loss: 0.6527 - val_acc: 0.7439\n",
      "Epoch 134/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6396 - acc: 0.7498Epoch 00133: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6395 - acc: 0.7498 - val_loss: 0.6407 - val_acc: 0.7545\n",
      "Epoch 135/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7408 - acc: 0.7334Epoch 00134: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7410 - acc: 0.7333 - val_loss: 0.8512 - val_acc: 0.6821\n",
      "Epoch 136/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7602 - acc: 0.7131Epoch 00135: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.7602 - acc: 0.7131 - val_loss: 0.7124 - val_acc: 0.7249\n",
      "Epoch 137/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6693 - acc: 0.7402Epoch 00136: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.6692 - acc: 0.7402 - val_loss: 0.6616 - val_acc: 0.7476\n",
      "Epoch 138/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7548 - acc: 0.7137Epoch 00137: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.7550 - acc: 0.7136 - val_loss: 0.8936 - val_acc: 0.6856\n",
      "Epoch 139/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7282 - acc: 0.7228Epoch 00138: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7281 - acc: 0.7229 - val_loss: 0.6788 - val_acc: 0.7372\n",
      "Epoch 140/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 1.0790 - acc: 0.5878Epoch 00139: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 1.0787 - acc: 0.5879 - val_loss: 0.9258 - val_acc: 0.6358\n",
      "Epoch 141/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8951 - acc: 0.6478Epoch 00140: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.8950 - acc: 0.6478 - val_loss: 0.8424 - val_acc: 0.6596\n",
      "Epoch 142/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8178 - acc: 0.6652Epoch 00141: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.8177 - acc: 0.6653 - val_loss: 0.7994 - val_acc: 0.6723\n",
      "Epoch 143/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7787 - acc: 0.6768Epoch 00142: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7787 - acc: 0.6769 - val_loss: 0.7645 - val_acc: 0.6841\n",
      "Epoch 144/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7455 - acc: 0.6940Epoch 00143: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7455 - acc: 0.6940 - val_loss: 0.7372 - val_acc: 0.7027\n",
      "Epoch 145/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7373 - acc: 0.7051Epoch 00144: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7372 - acc: 0.7051 - val_loss: 0.7226 - val_acc: 0.7116\n",
      "Epoch 146/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7176 - acc: 0.7120Epoch 00145: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7175 - acc: 0.7120 - val_loss: 0.7042 - val_acc: 0.7184\n",
      "Epoch 147/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7179 - acc: 0.7113Epoch 00146: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7179 - acc: 0.7113 - val_loss: 0.7117 - val_acc: 0.7124\n",
      "Epoch 148/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6965 - acc: 0.7186Epoch 00147: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.6966 - acc: 0.7186 - val_loss: 0.6888 - val_acc: 0.7241\n",
      "Epoch 149/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6680 - acc: 0.7318Epoch 00148: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.6680 - acc: 0.7319 - val_loss: 0.7007 - val_acc: 0.7235\n",
      "Epoch 150/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6613 - acc: 0.7411Epoch 00149: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6612 - acc: 0.7411 - val_loss: 0.6444 - val_acc: 0.7489\n",
      "Epoch 151/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6652 - acc: 0.7428Epoch 00150: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6652 - acc: 0.7428 - val_loss: 0.6478 - val_acc: 0.7529\n",
      "Epoch 152/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6416 - acc: 0.7546Epoch 00151: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6416 - acc: 0.7546 - val_loss: 0.6508 - val_acc: 0.7493\n",
      "Epoch 153/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6580 - acc: 0.7421Epoch 00152: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6581 - acc: 0.7421 - val_loss: 0.6507 - val_acc: 0.7421\n",
      "Epoch 154/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6340 - acc: 0.7539Epoch 00153: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6339 - acc: 0.7539 - val_loss: 0.6318 - val_acc: 0.7634\n",
      "Epoch 155/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7786 - acc: 0.7202Epoch 00154: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7786 - acc: 0.7202 - val_loss: 0.7799 - val_acc: 0.7342\n",
      "Epoch 156/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7107 - acc: 0.7432Epoch 00155: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7105 - acc: 0.7432 - val_loss: 0.6640 - val_acc: 0.7581\n",
      "Epoch 157/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6695 - acc: 0.7616Epoch 00156: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6696 - acc: 0.7616 - val_loss: 0.6940 - val_acc: 0.7512\n",
      "Epoch 158/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6573 - acc: 0.7625Epoch 00157: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6573 - acc: 0.7625 - val_loss: 0.6386 - val_acc: 0.7723\n",
      "Epoch 159/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6374 - acc: 0.7731Epoch 00158: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6374 - acc: 0.7731 - val_loss: 0.6569 - val_acc: 0.7633\n",
      "Epoch 160/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6314 - acc: 0.7730Epoch 00159: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6313 - acc: 0.7730 - val_loss: 0.6744 - val_acc: 0.7698\n",
      "Epoch 161/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7271 - acc: 0.7444Epoch 00160: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7271 - acc: 0.7444 - val_loss: 0.7535 - val_acc: 0.7285\n",
      "Epoch 162/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7164 - acc: 0.7343Epoch 00161: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7165 - acc: 0.7343 - val_loss: 0.7607 - val_acc: 0.7240\n",
      "Epoch 163/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7630 - acc: 0.7105Epoch 00162: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7630 - acc: 0.7106 - val_loss: 0.7233 - val_acc: 0.7216\n",
      "Epoch 164/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7190 - acc: 0.7249Epoch 00163: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7190 - acc: 0.7249 - val_loss: 0.7091 - val_acc: 0.7321\n",
      "Epoch 165/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6688 - acc: 0.7409Epoch 00164: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6687 - acc: 0.7409 - val_loss: 0.6377 - val_acc: 0.7515\n",
      "Epoch 166/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7109 - acc: 0.7263Epoch 00165: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7108 - acc: 0.7263 - val_loss: 0.6825 - val_acc: 0.7349\n",
      "Epoch 167/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7078 - acc: 0.7299Epoch 00166: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7077 - acc: 0.7300 - val_loss: 0.6900 - val_acc: 0.7425\n",
      "Epoch 168/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6681 - acc: 0.7491Epoch 00167: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6680 - acc: 0.7492 - val_loss: 0.6476 - val_acc: 0.7605\n",
      "Epoch 169/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6764 - acc: 0.7431Epoch 00168: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6764 - acc: 0.7431 - val_loss: 0.6559 - val_acc: 0.7584\n",
      "Epoch 170/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6893 - acc: 0.7381Epoch 00169: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6893 - acc: 0.7381 - val_loss: 0.7071 - val_acc: 0.7306\n",
      "Epoch 171/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7470 - acc: 0.7247Epoch 00170: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7469 - acc: 0.7247 - val_loss: 0.7088 - val_acc: 0.7395\n",
      "Epoch 172/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7787 - acc: 0.7278Epoch 00171: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7788 - acc: 0.7278 - val_loss: 0.8452 - val_acc: 0.6963\n",
      "Epoch 173/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7921 - acc: 0.7265Epoch 00172: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7920 - acc: 0.7265 - val_loss: 0.7427 - val_acc: 0.7412\n",
      "Epoch 174/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6803 - acc: 0.7579Epoch 00173: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6801 - acc: 0.7579 - val_loss: 0.6488 - val_acc: 0.7732\n",
      "Epoch 175/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7746 - acc: 0.7362Epoch 00174: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7751 - acc: 0.7360 - val_loss: 1.0684 - val_acc: 0.6534\n",
      "Epoch 176/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8246 - acc: 0.6977Epoch 00175: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.8243 - acc: 0.6978 - val_loss: 0.7381 - val_acc: 0.7256\n",
      "Epoch 177/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8127 - acc: 0.7044Epoch 00176: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.8126 - acc: 0.7044 - val_loss: 0.7898 - val_acc: 0.7359\n",
      "Epoch 178/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7129 - acc: 0.7550Epoch 00177: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7127 - acc: 0.7550 - val_loss: 0.6782 - val_acc: 0.7683\n",
      "Epoch 179/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6455 - acc: 0.7761Epoch 00178: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6455 - acc: 0.7762 - val_loss: 0.6245 - val_acc: 0.7847\n",
      "Epoch 180/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6434 - acc: 0.7837Epoch 00179: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6435 - acc: 0.7837 - val_loss: 0.7022 - val_acc: 0.7595\n",
      "Epoch 181/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7161 - acc: 0.7525Epoch 00180: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7160 - acc: 0.7525 - val_loss: 0.6525 - val_acc: 0.7785\n",
      "Epoch 182/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6251 - acc: 0.7809Epoch 00181: val_loss improved from 0.61986 to 0.60785, saving model to saved_models/weights.model_final5.hdf5\n",
      "110288/110288 [==============================] - 127s - loss: 0.6251 - acc: 0.7809 - val_loss: 0.6079 - val_acc: 0.7858\n",
      "Epoch 183/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6526 - acc: 0.7771Epoch 00182: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6525 - acc: 0.7771 - val_loss: 0.6473 - val_acc: 0.7794\n",
      "Epoch 184/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6662 - acc: 0.7614Epoch 00183: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6661 - acc: 0.7614 - val_loss: 0.6630 - val_acc: 0.7682\n",
      "Epoch 185/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7731 - acc: 0.7402Epoch 00184: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7732 - acc: 0.7403 - val_loss: 0.7363 - val_acc: 0.7413\n",
      "Epoch 186/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7250 - acc: 0.7376Epoch 00185: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110288/110288 [==============================] - 126s - loss: 0.7252 - acc: 0.7375 - val_loss: 0.8020 - val_acc: 0.6805\n",
      "Epoch 187/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7037 - acc: 0.7108Epoch 00186: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7037 - acc: 0.7108 - val_loss: 0.6879 - val_acc: 0.7203\n",
      "Epoch 188/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6579 - acc: 0.7332Epoch 00187: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6577 - acc: 0.7333 - val_loss: 0.6429 - val_acc: 0.7440\n",
      "Epoch 189/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6172 - acc: 0.7561Epoch 00188: val_loss improved from 0.60785 to 0.60445, saving model to saved_models/weights.model_final5.hdf5\n",
      "110288/110288 [==============================] - 127s - loss: 0.6172 - acc: 0.7561 - val_loss: 0.6045 - val_acc: 0.7680\n",
      "Epoch 190/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6446 - acc: 0.7578Epoch 00189: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6445 - acc: 0.7579 - val_loss: 0.6274 - val_acc: 0.7701\n",
      "Epoch 191/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6355 - acc: 0.7711Epoch 00190: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6358 - acc: 0.7710 - val_loss: 0.7806 - val_acc: 0.7142\n",
      "Epoch 192/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6486 - acc: 0.7720Epoch 00191: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6483 - acc: 0.7721 - val_loss: 0.6139 - val_acc: 0.7905\n",
      "Epoch 193/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6227 - acc: 0.7751Epoch 00192: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6226 - acc: 0.7751 - val_loss: 0.6074 - val_acc: 0.7715\n",
      "Epoch 194/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6613 - acc: 0.7638Epoch 00193: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6611 - acc: 0.7639 - val_loss: 0.6728 - val_acc: 0.7768\n",
      "Epoch 195/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6305 - acc: 0.7790Epoch 00194: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6304 - acc: 0.7790 - val_loss: 0.6290 - val_acc: 0.7827\n",
      "Epoch 196/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6965 - acc: 0.7578Epoch 00195: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6966 - acc: 0.7578 - val_loss: 0.6651 - val_acc: 0.7668\n",
      "Epoch 197/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6279 - acc: 0.7867Epoch 00196: val_loss improved from 0.60445 to 0.60203, saving model to saved_models/weights.model_final5.hdf5\n",
      "110288/110288 [==============================] - 127s - loss: 0.6279 - acc: 0.7867 - val_loss: 0.6020 - val_acc: 0.7964\n",
      "Epoch 198/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6567 - acc: 0.7793Epoch 00197: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6567 - acc: 0.7793 - val_loss: 0.6819 - val_acc: 0.7619\n",
      "Epoch 199/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6314 - acc: 0.7752Epoch 00198: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6315 - acc: 0.7751 - val_loss: 0.7308 - val_acc: 0.7586\n",
      "Epoch 200/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6340 - acc: 0.7750Epoch 00199: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6339 - acc: 0.7750 - val_loss: 0.6058 - val_acc: 0.7837\n",
      "Epoch 201/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6896 - acc: 0.7657Epoch 00200: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6896 - acc: 0.7657 - val_loss: 0.6591 - val_acc: 0.7739\n",
      "Epoch 202/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6943 - acc: 0.7687Epoch 00201: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6944 - acc: 0.7687 - val_loss: 0.6103 - val_acc: 0.7854\n",
      "Epoch 203/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6012 - acc: 0.7887Epoch 00202: val_loss improved from 0.60203 to 0.59390, saving model to saved_models/weights.model_final5.hdf5\n",
      "110288/110288 [==============================] - 126s - loss: 0.6011 - acc: 0.7888 - val_loss: 0.5939 - val_acc: 0.7906\n",
      "Epoch 204/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5850 - acc: 0.7964Epoch 00203: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5853 - acc: 0.7964 - val_loss: 0.7347 - val_acc: 0.7839\n",
      "Epoch 205/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6700 - acc: 0.7575Epoch 00204: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6699 - acc: 0.7575 - val_loss: 0.6216 - val_acc: 0.7602\n",
      "Epoch 206/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6227 - acc: 0.7630Epoch 00205: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6228 - acc: 0.7630 - val_loss: 0.6415 - val_acc: 0.7594\n",
      "Epoch 207/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6571 - acc: 0.7596Epoch 00206: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6570 - acc: 0.7596 - val_loss: 0.6186 - val_acc: 0.7676\n",
      "Epoch 208/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5923 - acc: 0.7761Epoch 00207: val_loss improved from 0.59390 to 0.58260, saving model to saved_models/weights.model_final5.hdf5\n",
      "110288/110288 [==============================] - 126s - loss: 0.5923 - acc: 0.7761 - val_loss: 0.5826 - val_acc: 0.7846\n",
      "Epoch 209/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5629 - acc: 0.7904Epoch 00208: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5630 - acc: 0.7904 - val_loss: 0.6146 - val_acc: 0.7759\n",
      "Epoch 210/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6569 - acc: 0.7666Epoch 00209: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6569 - acc: 0.7667 - val_loss: 0.5904 - val_acc: 0.7896\n",
      "Epoch 211/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7093 - acc: 0.7197Epoch 00210: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7099 - acc: 0.7190 - val_loss: 1.0544 - val_acc: 0.3067\n",
      "Epoch 212/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.9353 - acc: 0.3586Epoch 00211: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.9353 - acc: 0.3588 - val_loss: 0.9082 - val_acc: 0.4148\n",
      "Epoch 213/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7883 - acc: 0.6521Epoch 00212: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7879 - acc: 0.6523 - val_loss: 0.6370 - val_acc: 0.7481\n",
      "Epoch 214/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6110 - acc: 0.7605Epoch 00213: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6111 - acc: 0.7605 - val_loss: 0.6106 - val_acc: 0.7623\n",
      "Epoch 215/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6068 - acc: 0.7675Epoch 00214: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6067 - acc: 0.7676 - val_loss: 0.5967 - val_acc: 0.7766\n",
      "Epoch 216/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5956 - acc: 0.7744Epoch 00215: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5957 - acc: 0.7744 - val_loss: 0.5942 - val_acc: 0.7696\n",
      "Epoch 217/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5731 - acc: 0.7776Epoch 00216: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5730 - acc: 0.7776 - val_loss: 0.6120 - val_acc: 0.7675\n",
      "Epoch 218/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6478 - acc: 0.7661Epoch 00217: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6477 - acc: 0.7661 - val_loss: 0.6340 - val_acc: 0.7698\n",
      "Epoch 219/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5762 - acc: 0.7842Epoch 00218: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5762 - acc: 0.7841 - val_loss: 0.5859 - val_acc: 0.7825\n",
      "Epoch 220/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6088 - acc: 0.7810Epoch 00219: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6097 - acc: 0.7808 - val_loss: 1.1520 - val_acc: 0.6681\n",
      "Epoch 221/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7071 - acc: 0.7614Epoch 00220: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7070 - acc: 0.7615 - val_loss: 0.6217 - val_acc: 0.7834\n",
      "Epoch 222/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6426 - acc: 0.7763Epoch 00221: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6430 - acc: 0.7762 - val_loss: 0.8923 - val_acc: 0.7014\n",
      "Epoch 223/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7096 - acc: 0.7475Epoch 00222: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7095 - acc: 0.7475 - val_loss: 0.6725 - val_acc: 0.7693\n",
      "Epoch 224/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6084 - acc: 0.7820Epoch 00223: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6085 - acc: 0.7820 - val_loss: 0.5921 - val_acc: 0.7919\n",
      "Epoch 225/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6249 - acc: 0.7754Epoch 00224: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6249 - acc: 0.7754 - val_loss: 0.6271 - val_acc: 0.7724\n",
      "Epoch 226/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6680 - acc: 0.7584Epoch 00225: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6680 - acc: 0.7584 - val_loss: 0.6409 - val_acc: 0.7672\n",
      "Epoch 227/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6018 - acc: 0.7792Epoch 00226: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6017 - acc: 0.7792 - val_loss: 0.5857 - val_acc: 0.7887\n",
      "Epoch 228/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5665 - acc: 0.7946Epoch 00227: val_loss improved from 0.58260 to 0.55648, saving model to saved_models/weights.model_final5.hdf5\n",
      "110288/110288 [==============================] - 127s - loss: 0.5663 - acc: 0.7946 - val_loss: 0.5565 - val_acc: 0.8019\n",
      "Epoch 229/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5550 - acc: 0.7990Epoch 00228: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.5551 - acc: 0.7990 - val_loss: 0.6616 - val_acc: 0.7819\n",
      "Epoch 230/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8041 - acc: 0.7090Epoch 00229: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.8039 - acc: 0.7090 - val_loss: 0.6888 - val_acc: 0.7386\n",
      "Epoch 231/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6364 - acc: 0.7626Epoch 00230: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6363 - acc: 0.7626 - val_loss: 0.6056 - val_acc: 0.7828\n",
      "Epoch 232/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7105 - acc: 0.7350Epoch 00231: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7103 - acc: 0.7351 - val_loss: 0.6364 - val_acc: 0.7647\n",
      "Epoch 233/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6582 - acc: 0.7587Epoch 00232: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6581 - acc: 0.7587 - val_loss: 0.6348 - val_acc: 0.7684\n",
      "Epoch 234/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6215 - acc: 0.7718Epoch 00233: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6214 - acc: 0.7719 - val_loss: 0.6052 - val_acc: 0.7841\n",
      "Epoch 235/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6274 - acc: 0.7799Epoch 00234: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6276 - acc: 0.7799 - val_loss: 0.7200 - val_acc: 0.7598\n",
      "Epoch 236/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6471 - acc: 0.7752Epoch 00235: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6468 - acc: 0.7753 - val_loss: 0.6161 - val_acc: 0.7880\n",
      "Epoch 237/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5854 - acc: 0.7942Epoch 00236: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5854 - acc: 0.7942 - val_loss: 0.5739 - val_acc: 0.8001\n",
      "Epoch 238/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5873 - acc: 0.7902Epoch 00237: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5874 - acc: 0.7902 - val_loss: 0.6006 - val_acc: 0.7829\n",
      "Epoch 239/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6173 - acc: 0.7759Epoch 00238: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6175 - acc: 0.7758 - val_loss: 0.7724 - val_acc: 0.7242\n",
      "Epoch 240/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6468 - acc: 0.7733Epoch 00239: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6469 - acc: 0.7733 - val_loss: 0.7134 - val_acc: 0.7413\n",
      "Epoch 241/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6674 - acc: 0.7641Epoch 00240: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6672 - acc: 0.7641 - val_loss: 0.6338 - val_acc: 0.7786\n",
      "Epoch 242/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6087 - acc: 0.7879Epoch 00241: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6086 - acc: 0.7879 - val_loss: 0.6073 - val_acc: 0.7906\n",
      "Epoch 243/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7828 - acc: 0.7314Epoch 00242: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7826 - acc: 0.7315 - val_loss: 0.6923 - val_acc: 0.7553\n",
      "Epoch 244/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6571 - acc: 0.7684Epoch 00243: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6570 - acc: 0.7684 - val_loss: 0.6524 - val_acc: 0.7760\n",
      "Epoch 245/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7117 - acc: 0.7587Epoch 00244: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7119 - acc: 0.7587 - val_loss: 0.7604 - val_acc: 0.7352\n",
      "Epoch 246/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6926 - acc: 0.7482Epoch 00245: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6926 - acc: 0.7482 - val_loss: 0.6656 - val_acc: 0.7568\n",
      "Epoch 247/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6434 - acc: 0.7636Epoch 00246: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6434 - acc: 0.7637 - val_loss: 0.6301 - val_acc: 0.7704\n",
      "Epoch 248/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6118 - acc: 0.7773Epoch 00247: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110288/110288 [==============================] - 126s - loss: 0.6118 - acc: 0.7774 - val_loss: 0.6135 - val_acc: 0.7825\n",
      "Epoch 249/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6023 - acc: 0.7838Epoch 00248: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6023 - acc: 0.7838 - val_loss: 0.5876 - val_acc: 0.7894\n",
      "Epoch 250/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5859 - acc: 0.7920Epoch 00249: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5860 - acc: 0.7920 - val_loss: 0.6378 - val_acc: 0.7879\n",
      "Epoch 251/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6224 - acc: 0.7653Epoch 00250: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6228 - acc: 0.7651 - val_loss: 0.8132 - val_acc: 0.6678\n",
      "Epoch 252/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7316 - acc: 0.7128Epoch 00251: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7315 - acc: 0.7128 - val_loss: 0.7151 - val_acc: 0.7255\n",
      "Epoch 253/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6450 - acc: 0.7417Epoch 00252: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6449 - acc: 0.7418 - val_loss: 0.6301 - val_acc: 0.7519\n",
      "Epoch 254/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6458 - acc: 0.7424Epoch 00253: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6457 - acc: 0.7424 - val_loss: 0.6387 - val_acc: 0.7447\n",
      "Epoch 255/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6042 - acc: 0.7587Epoch 00254: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6042 - acc: 0.7587 - val_loss: 0.6058 - val_acc: 0.7611\n",
      "Epoch 256/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5842 - acc: 0.7755Epoch 00255: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5843 - acc: 0.7755 - val_loss: 0.5780 - val_acc: 0.7862\n",
      "Epoch 257/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6278 - acc: 0.7715Epoch 00256: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6277 - acc: 0.7715 - val_loss: 0.6028 - val_acc: 0.7737\n",
      "Epoch 258/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5873 - acc: 0.7818Epoch 00257: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5874 - acc: 0.7819 - val_loss: 0.5671 - val_acc: 0.7926\n",
      "Epoch 259/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6028 - acc: 0.7840Epoch 00258: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6029 - acc: 0.7840 - val_loss: 0.6448 - val_acc: 0.7835\n",
      "Epoch 260/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6672 - acc: 0.7806Epoch 00259: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6675 - acc: 0.7806 - val_loss: 0.7492 - val_acc: 0.7603\n",
      "Epoch 261/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6524 - acc: 0.7790Epoch 00260: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6524 - acc: 0.7791 - val_loss: 0.6143 - val_acc: 0.7916\n",
      "Epoch 262/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6348 - acc: 0.7847Epoch 00261: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6351 - acc: 0.7847 - val_loss: 0.7398 - val_acc: 0.7533\n",
      "Epoch 263/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6638 - acc: 0.7619Epoch 00262: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6637 - acc: 0.7619 - val_loss: 0.6313 - val_acc: 0.7716\n",
      "Epoch 264/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6704 - acc: 0.7654Epoch 00263: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6704 - acc: 0.7654 - val_loss: 0.6721 - val_acc: 0.7596\n",
      "Epoch 265/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6332 - acc: 0.7680Epoch 00264: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6333 - acc: 0.7680 - val_loss: 0.6677 - val_acc: 0.7604\n",
      "Epoch 266/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6249 - acc: 0.7712Epoch 00265: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6250 - acc: 0.7711 - val_loss: 0.6188 - val_acc: 0.7828\n",
      "Epoch 267/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6066 - acc: 0.7797Epoch 00266: val_loss did not improve\n",
      "110288/110288 [==============================] - 127s - loss: 0.6065 - acc: 0.7797 - val_loss: 0.6263 - val_acc: 0.7735\n",
      "Epoch 268/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6890 - acc: 0.7285Epoch 00267: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6892 - acc: 0.7284 - val_loss: 0.7814 - val_acc: 0.6608\n",
      "Epoch 269/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6463 - acc: 0.7474Epoch 00268: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6462 - acc: 0.7475 - val_loss: 0.6093 - val_acc: 0.7775\n",
      "Epoch 270/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8281 - acc: 0.5069Epoch 00269: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.8280 - acc: 0.5070 - val_loss: 0.8335 - val_acc: 0.5060\n",
      "Epoch 271/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5439 - acc: 0.8130Epoch 00295: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5438 - acc: 0.8130 - val_loss: 0.5572 - val_acc: 0.8169\n",
      "Epoch 297/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5988 - acc: 0.7852Epoch 00296: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5987 - acc: 0.7852 - val_loss: 0.5832 - val_acc: 0.7825\n",
      "Epoch 298/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6157 - acc: 0.7848Epoch 00297: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6158 - acc: 0.7848 - val_loss: 0.6582 - val_acc: 0.7721\n",
      "Epoch 299/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6086 - acc: 0.7825Epoch 00298: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6085 - acc: 0.7825 - val_loss: 0.5760 - val_acc: 0.8011\n",
      "Epoch 300/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6660 - acc: 0.7497Epoch 00299: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6657 - acc: 0.7498 - val_loss: 0.6059 - val_acc: 0.7817\n",
      "Epoch 301/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5660 - acc: 0.7946Epoch 00300: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5657 - acc: 0.7947 - val_loss: 0.5538 - val_acc: 0.8049\n",
      "Epoch 302/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5524 - acc: 0.8097Epoch 00301: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5524 - acc: 0.8097 - val_loss: 0.6241 - val_acc: 0.8152\n",
      "Epoch 303/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5768 - acc: 0.8106Epoch 00302: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5768 - acc: 0.8106 - val_loss: 0.5726 - val_acc: 0.8093\n",
      "Epoch 304/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6024 - acc: 0.7900Epoch 00303: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6022 - acc: 0.7900 - val_loss: 0.5723 - val_acc: 0.7958\n",
      "Epoch 305/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5933 - acc: 0.7824Epoch 00314: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110288/110288 [==============================] - 126s - loss: 0.5932 - acc: 0.7825 - val_loss: 0.5745 - val_acc: 0.7900\n",
      "Epoch 316/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5492 - acc: 0.7961Epoch 00315: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5491 - acc: 0.7961 - val_loss: 0.5445 - val_acc: 0.8030\n",
      "Epoch 317/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5198 - acc: 0.8107Epoch 00316: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5199 - acc: 0.8107 - val_loss: 0.5210 - val_acc: 0.8154\n",
      "Epoch 318/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5040 - acc: 0.8179Epoch 00317: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5041 - acc: 0.8179 - val_loss: 0.5239 - val_acc: 0.8188\n",
      "Epoch 319/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5684 - acc: 0.8081Epoch 00318: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5689 - acc: 0.8080 - val_loss: 0.9063 - val_acc: 0.7377\n",
      "Epoch 320/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6515 - acc: 0.7781Epoch 00319: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6513 - acc: 0.7781 - val_loss: 0.5907 - val_acc: 0.7909\n",
      "Epoch 321/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6038 - acc: 0.7832Epoch 00320: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6038 - acc: 0.7832 - val_loss: 0.5916 - val_acc: 0.7896\n",
      "Epoch 322/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5874 - acc: 0.7931Epoch 00321: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5873 - acc: 0.7932 - val_loss: 0.5672 - val_acc: 0.7999\n",
      "Epoch 323/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6098 - acc: 0.7833Epoch 00322: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6096 - acc: 0.7833 - val_loss: 0.5715 - val_acc: 0.7954\n",
      "Epoch 324/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5441 - acc: 0.8016Epoch 00323: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5439 - acc: 0.8017 - val_loss: 0.5415 - val_acc: 0.8061\n",
      "Epoch 325/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5641 - acc: 0.7992Epoch 00324: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5640 - acc: 0.7993 - val_loss: 0.5519 - val_acc: 0.8053\n",
      "Epoch 326/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6088 - acc: 0.7844Epoch 00325: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6088 - acc: 0.7843 - val_loss: 0.6314 - val_acc: 0.7692\n",
      "Epoch 327/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5840 - acc: 0.7831Epoch 00326: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5840 - acc: 0.7831 - val_loss: 0.5713 - val_acc: 0.7927\n",
      "Epoch 328/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6087 - acc: 0.7856Epoch 00327: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6098 - acc: 0.7855 - val_loss: 1.1055 - val_acc: 0.7101\n",
      "Epoch 329/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8523 - acc: 0.7758Epoch 00328: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.8518 - acc: 0.7759 - val_loss: 0.5831 - val_acc: 0.7970\n",
      "Epoch 330/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6462 - acc: 0.7669Epoch 00329: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6464 - acc: 0.7668 - val_loss: 0.6946 - val_acc: 0.7398\n",
      "Epoch 331/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6206 - acc: 0.7606Epoch 00330: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6204 - acc: 0.7607 - val_loss: 0.5973 - val_acc: 0.7726\n",
      "Epoch 332/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5713 - acc: 0.7821Epoch 00331: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5713 - acc: 0.7821 - val_loss: 0.5635 - val_acc: 0.7903\n",
      "Epoch 333/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5450 - acc: 0.7952Epoch 00332: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5449 - acc: 0.7952 - val_loss: 0.5421 - val_acc: 0.7998\n",
      "Epoch 334/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5226 - acc: 0.8054Epoch 00333: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5225 - acc: 0.8054 - val_loss: 0.5264 - val_acc: 0.8095\n",
      "Epoch 335/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5796 - acc: 0.7969Epoch 00334: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5794 - acc: 0.7970 - val_loss: 0.5467 - val_acc: 0.8055\n",
      "Epoch 336/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5577 - acc: 0.8008Epoch 00335: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5577 - acc: 0.8007 - val_loss: 0.5762 - val_acc: 0.7926\n",
      "Epoch 337/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5300 - acc: 0.8048Epoch 00336: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5300 - acc: 0.8049 - val_loss: 0.5408 - val_acc: 0.8119\n",
      "Epoch 338/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5698 - acc: 0.7913Epoch 00337: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5697 - acc: 0.7913 - val_loss: 0.5575 - val_acc: 0.7951\n",
      "Epoch 339/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5405 - acc: 0.8018Epoch 00338: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5406 - acc: 0.8018 - val_loss: 0.5351 - val_acc: 0.8093\n",
      "Epoch 340/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5423 - acc: 0.8130Epoch 00339: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5423 - acc: 0.8130 - val_loss: 0.5569 - val_acc: 0.8043\n",
      "Epoch 341/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5327 - acc: 0.8090Epoch 00340: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5326 - acc: 0.8090 - val_loss: 0.5196 - val_acc: 0.8204\n",
      "Epoch 342/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5491 - acc: 0.8049Epoch 00341: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5491 - acc: 0.8050 - val_loss: 0.5333 - val_acc: 0.8170\n",
      "Epoch 343/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5168 - acc: 0.8183Epoch 00342: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5169 - acc: 0.8183 - val_loss: 0.5392 - val_acc: 0.8150\n",
      "Epoch 344/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5188 - acc: 0.8180Epoch 00343: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5189 - acc: 0.8180 - val_loss: 0.5626 - val_acc: 0.7992\n",
      "Epoch 345/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5299 - acc: 0.7990Epoch 00344: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5298 - acc: 0.7991 - val_loss: 0.5180 - val_acc: 0.8057\n",
      "Epoch 346/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5031 - acc: 0.8111Epoch 00345: val_loss improved from 0.51675 to 0.50590, saving model to saved_models/weights.model_final5.hdf5\n",
      "110288/110288 [==============================] - 127s - loss: 0.5030 - acc: 0.8111 - val_loss: 0.5059 - val_acc: 0.8192\n",
      "Epoch 347/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5597 - acc: 0.8088Epoch 00346: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5598 - acc: 0.8087 - val_loss: 0.6089 - val_acc: 0.7915\n",
      "Epoch 348/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6009 - acc: 0.7912Epoch 00347: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6008 - acc: 0.7912 - val_loss: 0.5825 - val_acc: 0.7990\n",
      "Epoch 349/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5473 - acc: 0.8021Epoch 00348: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5473 - acc: 0.8021 - val_loss: 0.5404 - val_acc: 0.8098\n",
      "Epoch 350/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5236 - acc: 0.8075Epoch 00349: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5237 - acc: 0.8075 - val_loss: 0.5713 - val_acc: 0.8004\n",
      "Epoch 351/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7463 - acc: 0.7708Epoch 00350: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7462 - acc: 0.7708 - val_loss: 0.6422 - val_acc: 0.7876\n",
      "Epoch 352/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6201 - acc: 0.7765Epoch 00351: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6199 - acc: 0.7765 - val_loss: 0.5990 - val_acc: 0.7840\n",
      "Epoch 353/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5712 - acc: 0.7934Epoch 00352: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5712 - acc: 0.7934 - val_loss: 0.5755 - val_acc: 0.8025\n",
      "Epoch 354/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5389 - acc: 0.8078Epoch 00353: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5390 - acc: 0.8078 - val_loss: 0.5465 - val_acc: 0.8057\n",
      "Epoch 355/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5174 - acc: 0.8114Epoch 00354: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5174 - acc: 0.8114 - val_loss: 0.5119 - val_acc: 0.8174\n",
      "Epoch 356/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5054 - acc: 0.8169Epoch 00355: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5053 - acc: 0.8169 - val_loss: 0.5167 - val_acc: 0.8151\n",
      "Epoch 357/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5075 - acc: 0.8108Epoch 00356: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5075 - acc: 0.8108 - val_loss: 0.5244 - val_acc: 0.8074\n",
      "Epoch 358/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4985 - acc: 0.8148Epoch 00357: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4986 - acc: 0.8148 - val_loss: 0.5262 - val_acc: 0.8175\n",
      "Epoch 359/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5496 - acc: 0.8089Epoch 00358: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5495 - acc: 0.8089 - val_loss: 0.5145 - val_acc: 0.8197\n",
      "Epoch 360/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4859 - acc: 0.8270Epoch 00359: val_loss improved from 0.50590 to 0.48177, saving model to saved_models/weights.model_final5.hdf5\n",
      "110288/110288 [==============================] - 126s - loss: 0.4857 - acc: 0.8271 - val_loss: 0.4818 - val_acc: 0.8335\n",
      "Epoch 361/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5125 - acc: 0.8298Epoch 00360: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5126 - acc: 0.8298 - val_loss: 0.5764 - val_acc: 0.8219\n",
      "Epoch 362/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5537 - acc: 0.8171Epoch 00361: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5536 - acc: 0.8171 - val_loss: 0.5581 - val_acc: 0.8172\n",
      "Epoch 363/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5423 - acc: 0.8243Epoch 00362: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5423 - acc: 0.8243 - val_loss: 0.5238 - val_acc: 0.8330\n",
      "Epoch 364/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5368 - acc: 0.8217Epoch 00363: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5369 - acc: 0.8216 - val_loss: 0.5140 - val_acc: 0.8238\n",
      "Epoch 365/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5021 - acc: 0.8250Epoch 00364: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5022 - acc: 0.8250 - val_loss: 0.5629 - val_acc: 0.8135\n",
      "Epoch 366/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5095 - acc: 0.8279Epoch 00365: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5095 - acc: 0.8279 - val_loss: 0.4988 - val_acc: 0.8310\n",
      "Epoch 367/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5219 - acc: 0.8259Epoch 00366: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5220 - acc: 0.8258 - val_loss: 0.5802 - val_acc: 0.8093\n",
      "Epoch 368/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5314 - acc: 0.8226Epoch 00367: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5314 - acc: 0.8226 - val_loss: 0.5200 - val_acc: 0.8317\n",
      "Epoch 369/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4938 - acc: 0.8328Epoch 00368: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4938 - acc: 0.8328 - val_loss: 0.5072 - val_acc: 0.8363\n",
      "Epoch 370/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4785 - acc: 0.8398Epoch 00369: val_loss improved from 0.48177 to 0.47792, saving model to saved_models/weights.model_final5.hdf5\n",
      "110288/110288 [==============================] - 126s - loss: 0.4785 - acc: 0.8399 - val_loss: 0.4779 - val_acc: 0.8440\n",
      "Epoch 371/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4876 - acc: 0.8380Epoch 00370: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4875 - acc: 0.8380 - val_loss: 0.5125 - val_acc: 0.8298\n",
      "Epoch 372/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5070 - acc: 0.8278Epoch 00371: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5071 - acc: 0.8278 - val_loss: 0.5089 - val_acc: 0.8308\n",
      "Epoch 373/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5191 - acc: 0.8307Epoch 00372: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5190 - acc: 0.8307 - val_loss: 0.5135 - val_acc: 0.8354\n",
      "Epoch 374/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5223 - acc: 0.8221Epoch 00373: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5223 - acc: 0.8221 - val_loss: 0.5097 - val_acc: 0.8268\n",
      "Epoch 375/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4855 - acc: 0.8327Epoch 00374: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4854 - acc: 0.8328 - val_loss: 0.4959 - val_acc: 0.8276\n",
      "Epoch 376/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5010 - acc: 0.8217Epoch 00375: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5010 - acc: 0.8217 - val_loss: 0.5080 - val_acc: 0.8213\n",
      "Epoch 377/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5058 - acc: 0.8253Epoch 00376: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5061 - acc: 0.8253 - val_loss: 0.6079 - val_acc: 0.8286\n",
      "Epoch 378/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5066 - acc: 0.8363Epoch 00377: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5068 - acc: 0.8363 - val_loss: 0.5058 - val_acc: 0.8359\n",
      "Epoch 379/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4910 - acc: 0.8381Epoch 00378: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4909 - acc: 0.8381 - val_loss: 0.4830 - val_acc: 0.8397\n",
      "Epoch 380/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4628 - acc: 0.8448Epoch 00379: val_loss improved from 0.47792 to 0.46114, saving model to saved_models/weights.model_final5.hdf5\n",
      "110288/110288 [==============================] - 127s - loss: 0.4628 - acc: 0.8448 - val_loss: 0.4611 - val_acc: 0.8473\n",
      "Epoch 381/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4651 - acc: 0.8422Epoch 00380: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4652 - acc: 0.8422 - val_loss: 0.5296 - val_acc: 0.8281\n",
      "Epoch 382/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4735 - acc: 0.8415Epoch 00381: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4734 - acc: 0.8415 - val_loss: 0.4623 - val_acc: 0.8474\n",
      "Epoch 383/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4546 - acc: 0.8516Epoch 00382: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4545 - acc: 0.8516 - val_loss: 0.4646 - val_acc: 0.8588\n",
      "Epoch 384/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5257 - acc: 0.8269Epoch 00383: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5258 - acc: 0.8269 - val_loss: 0.4939 - val_acc: 0.8372\n",
      "Epoch 385/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4750 - acc: 0.8414Epoch 00384: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4749 - acc: 0.8414 - val_loss: 0.4806 - val_acc: 0.8472\n",
      "Epoch 386/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4499 - acc: 0.8528Epoch 00385: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4499 - acc: 0.8528 - val_loss: 0.4681 - val_acc: 0.8517\n",
      "Epoch 387/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4433 - acc: 0.8565Epoch 00386: val_loss improved from 0.46114 to 0.44267, saving model to saved_models/weights.model_final5.hdf5\n",
      "110288/110288 [==============================] - 127s - loss: 0.4433 - acc: 0.8566 - val_loss: 0.4427 - val_acc: 0.8599\n",
      "Epoch 388/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4566 - acc: 0.8523Epoch 00387: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4565 - acc: 0.8523 - val_loss: 0.4555 - val_acc: 0.8523\n",
      "Epoch 389/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4427 - acc: 0.8581Epoch 00388: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4427 - acc: 0.8581 - val_loss: 0.4651 - val_acc: 0.8556\n",
      "Epoch 390/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4987 - acc: 0.8546Epoch 00389: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4988 - acc: 0.8546 - val_loss: 0.5720 - val_acc: 0.8385\n",
      "Epoch 391/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4952 - acc: 0.8489Epoch 00390: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4954 - acc: 0.8489 - val_loss: 0.5781 - val_acc: 0.8413\n",
      "Epoch 392/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5562 - acc: 0.8397Epoch 00391: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5565 - acc: 0.8396 - val_loss: 0.6067 - val_acc: 0.8082\n",
      "Epoch 393/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5344 - acc: 0.8325Epoch 00392: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5345 - acc: 0.8325 - val_loss: 0.5499 - val_acc: 0.8333\n",
      "Epoch 394/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5363 - acc: 0.8442Epoch 00393: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5363 - acc: 0.8442 - val_loss: 0.5607 - val_acc: 0.8410\n",
      "Epoch 395/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5818 - acc: 0.8259Epoch 00394: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5818 - acc: 0.8258 - val_loss: 0.5653 - val_acc: 0.8270\n",
      "Epoch 396/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5366 - acc: 0.8362Epoch 00395: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5366 - acc: 0.8362 - val_loss: 0.5390 - val_acc: 0.8422\n",
      "Epoch 397/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5243 - acc: 0.8501Epoch 00396: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5244 - acc: 0.8500 - val_loss: 0.5248 - val_acc: 0.8534\n",
      "Epoch 398/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5051 - acc: 0.8506Epoch 00397: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5051 - acc: 0.8506 - val_loss: 0.5051 - val_acc: 0.8499\n",
      "Epoch 399/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5572 - acc: 0.8334Epoch 00398: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5573 - acc: 0.8334 - val_loss: 0.5348 - val_acc: 0.8353\n",
      "Epoch 400/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5282 - acc: 0.8343Epoch 00399: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5282 - acc: 0.8343 - val_loss: 0.5690 - val_acc: 0.8238\n",
      "Epoch 401/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5186 - acc: 0.8354Epoch 00400: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5186 - acc: 0.8354 - val_loss: 0.5253 - val_acc: 0.8384\n",
      "Epoch 402/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5202 - acc: 0.8411Epoch 00401: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5203 - acc: 0.8411 - val_loss: 0.5249 - val_acc: 0.8401\n",
      "Epoch 403/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4849 - acc: 0.8490Epoch 00402: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4851 - acc: 0.8490 - val_loss: 0.4915 - val_acc: 0.8526\n",
      "Epoch 404/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4725 - acc: 0.8550Epoch 00403: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4725 - acc: 0.8550 - val_loss: 0.4639 - val_acc: 0.8613\n",
      "Epoch 405/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4721 - acc: 0.8576Epoch 00404: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4721 - acc: 0.8576 - val_loss: 0.5131 - val_acc: 0.8518\n",
      "Epoch 406/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5116 - acc: 0.8460Epoch 00405: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5116 - acc: 0.8460 - val_loss: 0.4922 - val_acc: 0.8523\n",
      "Epoch 407/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4677 - acc: 0.8573Epoch 00406: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4677 - acc: 0.8573 - val_loss: 0.4906 - val_acc: 0.8579\n",
      "Epoch 408/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4557 - acc: 0.8596Epoch 00407: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4557 - acc: 0.8596 - val_loss: 0.4644 - val_acc: 0.8622\n",
      "Epoch 409/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4841 - acc: 0.8483Epoch 00408: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4840 - acc: 0.8483 - val_loss: 0.4853 - val_acc: 0.8506\n",
      "Epoch 410/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5245 - acc: 0.8369Epoch 00409: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5245 - acc: 0.8369 - val_loss: 0.5589 - val_acc: 0.8333\n",
      "Epoch 411/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5334 - acc: 0.8354Epoch 00410: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5333 - acc: 0.8354 - val_loss: 0.5276 - val_acc: 0.8446\n",
      "Epoch 412/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6078 - acc: 0.8188Epoch 00411: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6078 - acc: 0.8188 - val_loss: 0.6163 - val_acc: 0.8170\n",
      "Epoch 413/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5844 - acc: 0.8203Epoch 00412: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5843 - acc: 0.8203 - val_loss: 0.5953 - val_acc: 0.8182\n",
      "Epoch 414/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5528 - acc: 0.8274Epoch 00413: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5527 - acc: 0.8275 - val_loss: 0.5378 - val_acc: 0.8348\n",
      "Epoch 415/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5179 - acc: 0.8393Epoch 00414: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5177 - acc: 0.8394 - val_loss: 0.5167 - val_acc: 0.8436\n",
      "Epoch 416/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5039 - acc: 0.8449Epoch 00415: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5038 - acc: 0.8449 - val_loss: 0.5036 - val_acc: 0.8490\n",
      "Epoch 417/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4839 - acc: 0.8515Epoch 00416: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4838 - acc: 0.8515 - val_loss: 0.4928 - val_acc: 0.8549\n",
      "Epoch 418/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4695 - acc: 0.8557Epoch 00417: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4695 - acc: 0.8557 - val_loss: 0.4707 - val_acc: 0.8578\n",
      "Epoch 419/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4728 - acc: 0.8534Epoch 00418: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4727 - acc: 0.8534 - val_loss: 0.4878 - val_acc: 0.8527\n",
      "Epoch 420/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4683 - acc: 0.8563Epoch 00419: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4683 - acc: 0.8563 - val_loss: 0.4701 - val_acc: 0.8590\n",
      "Epoch 421/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4874 - acc: 0.8485Epoch 00420: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4874 - acc: 0.8485 - val_loss: 0.5072 - val_acc: 0.8444\n",
      "Epoch 422/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4820 - acc: 0.8497Epoch 00421: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4820 - acc: 0.8497 - val_loss: 0.4859 - val_acc: 0.8485\n",
      "Epoch 423/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4825 - acc: 0.8498Epoch 00422: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4825 - acc: 0.8498 - val_loss: 0.4929 - val_acc: 0.8456\n",
      "Epoch 424/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6425 - acc: 0.7997Epoch 00423: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6426 - acc: 0.7996 - val_loss: 0.7127 - val_acc: 0.7655\n",
      "Epoch 425/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6400 - acc: 0.7787Epoch 00424: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6400 - acc: 0.7787 - val_loss: 0.6050 - val_acc: 0.7915\n",
      "Epoch 426/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5804 - acc: 0.7970Epoch 00425: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5802 - acc: 0.7970 - val_loss: 0.5699 - val_acc: 0.8057\n",
      "Epoch 427/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5456 - acc: 0.8104Epoch 00426: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5456 - acc: 0.8104 - val_loss: 0.5434 - val_acc: 0.8170\n",
      "Epoch 428/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5352 - acc: 0.8212Epoch 00427: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5352 - acc: 0.8212 - val_loss: 0.5298 - val_acc: 0.8255\n",
      "Epoch 429/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5117 - acc: 0.8273Epoch 00428: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5117 - acc: 0.8273 - val_loss: 0.5108 - val_acc: 0.8296\n",
      "Epoch 430/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5026 - acc: 0.8321Epoch 00429: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5026 - acc: 0.8321 - val_loss: 0.5070 - val_acc: 0.8330\n",
      "Epoch 431/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5368 - acc: 0.8290Epoch 00430: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5368 - acc: 0.8290 - val_loss: 0.5603 - val_acc: 0.8275\n",
      "Epoch 432/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5202 - acc: 0.8309Epoch 00431: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5202 - acc: 0.8309 - val_loss: 0.5043 - val_acc: 0.8364\n",
      "Epoch 433/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4905 - acc: 0.8400Epoch 00432: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4905 - acc: 0.8400 - val_loss: 0.4895 - val_acc: 0.8436\n",
      "Epoch 434/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4797 - acc: 0.8450Epoch 00433: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4795 - acc: 0.8450 - val_loss: 0.4796 - val_acc: 0.8429\n",
      "Epoch 435/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4685 - acc: 0.8459Epoch 00434: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4684 - acc: 0.8459 - val_loss: 0.4923 - val_acc: 0.8478\n",
      "Epoch 436/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5040 - acc: 0.8278Epoch 00435: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5040 - acc: 0.8278 - val_loss: 0.5108 - val_acc: 0.8360\n",
      "Epoch 437/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5110 - acc: 0.8380Epoch 00436: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5114 - acc: 0.8380 - val_loss: 0.6348 - val_acc: 0.8232\n",
      "Epoch 438/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5017 - acc: 0.8383Epoch 00437: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5016 - acc: 0.8383 - val_loss: 0.4847 - val_acc: 0.8440\n",
      "Epoch 439/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4710 - acc: 0.8466Epoch 00438: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4710 - acc: 0.8466 - val_loss: 0.4817 - val_acc: 0.8475\n",
      "Epoch 440/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4655 - acc: 0.8527Epoch 00439: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110288/110288 [==============================] - 126s - loss: 0.4656 - acc: 0.8527 - val_loss: 0.4662 - val_acc: 0.8558\n",
      "Epoch 441/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.9136 - acc: 0.4987Epoch 00440: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.9139 - acc: 0.4983 - val_loss: 1.1433 - val_acc: 0.3198\n",
      "Epoch 442/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.9387 - acc: 0.4240Epoch 00441: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.9386 - acc: 0.4241 - val_loss: 0.8491 - val_acc: 0.4788\n",
      "Epoch 443/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.8211 - acc: 0.5126Epoch 00442: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.8212 - acc: 0.5126 - val_loss: 0.8296 - val_acc: 0.5465\n",
      "Epoch 444/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.7924 - acc: 0.6334Epoch 00443: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.7922 - acc: 0.6337 - val_loss: 0.7716 - val_acc: 0.7820\n",
      "Epoch 445/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6852 - acc: 0.8110Epoch 00444: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6852 - acc: 0.8109 - val_loss: 0.6875 - val_acc: 0.7815\n",
      "Epoch 446/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5983 - acc: 0.7999Epoch 00445: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5981 - acc: 0.7999 - val_loss: 0.5625 - val_acc: 0.8121\n",
      "Epoch 447/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5452 - acc: 0.8146Epoch 00446: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5451 - acc: 0.8146 - val_loss: 0.5465 - val_acc: 0.8131\n",
      "Epoch 448/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5133 - acc: 0.8226Epoch 00447: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5134 - acc: 0.8226 - val_loss: 0.5084 - val_acc: 0.8268\n",
      "Epoch 449/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5052 - acc: 0.8259Epoch 00448: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5051 - acc: 0.8259 - val_loss: 0.4972 - val_acc: 0.8304\n",
      "Epoch 450/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4908 - acc: 0.8325Epoch 00449: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4912 - acc: 0.8325 - val_loss: 0.6808 - val_acc: 0.8241\n",
      "Epoch 451/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5609 - acc: 0.8334Epoch 00450: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5607 - acc: 0.8334 - val_loss: 0.5225 - val_acc: 0.8395\n",
      "Epoch 452/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4902 - acc: 0.8457Epoch 00451: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4902 - acc: 0.8458 - val_loss: 0.4924 - val_acc: 0.8445\n",
      "Epoch 453/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4825 - acc: 0.8500Epoch 00452: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4824 - acc: 0.8500 - val_loss: 0.4933 - val_acc: 0.8516\n",
      "Epoch 454/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5224 - acc: 0.8407Epoch 00453: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5224 - acc: 0.8407 - val_loss: 0.5341 - val_acc: 0.8370\n",
      "Epoch 455/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5000 - acc: 0.8433Epoch 00454: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4999 - acc: 0.8433 - val_loss: 0.4945 - val_acc: 0.8460\n",
      "Epoch 456/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4693 - acc: 0.8514Epoch 00455: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4692 - acc: 0.8514 - val_loss: 0.4795 - val_acc: 0.8550\n",
      "Epoch 457/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.6368 - acc: 0.8042Epoch 00456: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.6367 - acc: 0.8042 - val_loss: 0.6241 - val_acc: 0.8041\n",
      "Epoch 458/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5678 - acc: 0.8159Epoch 00457: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5679 - acc: 0.8159 - val_loss: 0.5513 - val_acc: 0.8261\n",
      "Epoch 459/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.5213 - acc: 0.8301Epoch 00458: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.5212 - acc: 0.8301 - val_loss: 0.5225 - val_acc: 0.8357\n",
      "Epoch 460/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4946 - acc: 0.8423Epoch 00459: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4948 - acc: 0.8423 - val_loss: 0.5017 - val_acc: 0.8443\n",
      "Epoch 461/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4762 - acc: 0.8512Epoch 00460: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4762 - acc: 0.8512 - val_loss: 0.4865 - val_acc: 0.8545\n",
      "Epoch 462/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4639 - acc: 0.8575Epoch 00461: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4640 - acc: 0.8575 - val_loss: 0.5181 - val_acc: 0.8410\n",
      "Epoch 463/500\n",
      "110080/110288 [============================>.] - ETA: 0s - loss: 0.4927 - acc: 0.8465Epoch 00462: val_loss did not improve\n",
      "110288/110288 [==============================] - 126s - loss: 0.4928 - acc: 0.8465 - val_loss: 0.4758 - val_acc: 0.8517\n",
      "Epoch 00462: early stopping\n"
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.model_final5.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "earlystopper = EarlyStopping(monitor='val_loss', min_delta=0.005, patience=75, verbose=1, mode='auto')\n",
    "\n",
    "history = model_rnn_final.fit(tmp_x, preproc_french_sentences, batch_size=256, \n",
    "                    epochs=500, validation_split=0.2, callbacks=[checkpointer, earlystopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'loss', 'val_acc', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzKzLScHMzLqcFMzMrMtJwczMuv4fIr5VVWjP9FYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f373e90af28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nb+zIci2//k0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f373e9213c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "      \n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline   \n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "fig1 = plt.figure()\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model5 accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "fig2 = plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model5 loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1.savefig('model5_epochs462_acc.png')\n",
    "fig2.savefig('model5_epochs462_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    "display(Image(filename=\"model2_epochs140_acc.png\"))\n",
    "display(Image(filename=\"model2_epochs140_loss.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn_final.load_weights('saved_models/weights.model_final5.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction (IMPLEMENTATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 26 127 100 111 112 101   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0]\n",
      " [ 17  23   1   8  67   4  39   7   3   1  55   2  44   0   0   0   0   0\n",
      "    0   0   0]]\n",
      "Sample 1:\n",
      "il a vu un vieux camion blanc <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Il a vu un vieux camion jaune\n",
      "Sample 2:\n",
      "new jersey est parfois calme en l'automne et il et il est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def final_predictions(x, y, x_tk, y_tk):\n",
    "    \"\"\"\n",
    "    Gets predictions using the final model\n",
    "    :param x: Preprocessed English data\n",
    "    :param y: Preprocessed French data\n",
    "    :param x_tk: English tokenizer\n",
    "    :param y_tk: French tokenizer\n",
    "    \"\"\"\n",
    "    # TODO: Train neural network using model_final\n",
    "    model = model_rnn_final\n",
    "\n",
    "    x = pad(x, y.shape[1])\n",
    "    \n",
    "    ## DON'T EDIT ANYTHING BELOW THIS LINE\n",
    "    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "    y_id_to_word[0] = '<PAD>'\n",
    "\n",
    "    sentence = 'he saw a old yellow truck'\n",
    "    sentence = [x_tk.word_index[word] for word in sentence.split()]\n",
    "    sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n",
    "    sentences = np.array([sentence[0], x[0]])\n",
    "    print(sentences)\n",
    "    predictions = model.predict(sentences, len(sentences))\n",
    "\n",
    "    print('Sample 1:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n",
    "    print('Il a vu un vieux camion jaune')\n",
    "    print('Sample 2:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in y[0]]))\n",
    "\n",
    "\n",
    "final_predictions(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGLISH: his favorite fruit is the orange , but my favorite is the grape .\n",
      "FRENCH: son fruit préféré est l'orange mais mon préféré est le chaux <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "ENGLISH: he saw a old yellow truck\n",
      "FRENCH: est est en en en mais <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "# Custom predictions\n",
    "    \n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "sentence_index = 5\n",
    "print(\"ENGLISH:\", english_sentences[sentence_index])\n",
    "print(\"FRENCH:\", logits_to_text(model_rnn_final.predict(tmp_x[sentence_index:sentence_index + 1])[0], french_tokenizer))\n",
    "\n",
    "\n",
    "sentences = ['he saw a old yellow truck', 'it is nice to see you today']\n",
    "preproc_sentences, blank, sentence_tokenizer, blank_tokenizer =\\\n",
    "    preprocess(sentences, ['test'])\n",
    "     \n",
    "tmp_x = pad(preproc_sentences, preproc_french_sentences.shape[1])\n",
    "sentence_index = 0\n",
    "print(\"ENGLISH:\", sentences[sentence_index])\n",
    "print(\"FRENCH:\", logits_to_text(model_rnn_final.predict(tmp_x[sentence_index:sentence_index + 1])[0], french_tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "When you are ready to submit your project, do the following steps:\n",
    "1. Ensure you pass all points on the [rubric](https://review.udacity.com/#!/rubrics/1004/view).\n",
    "2. Submit the following in a zip file.\n",
    "  - `helper.py`\n",
    "  - `machine_translation.ipynb`\n",
    "  - `machine_translation.html`\n",
    "    - You can export the notebook by navigating to **File -> Download as -> HTML (.html)**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
